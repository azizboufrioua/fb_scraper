{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85d460b8",
   "metadata": {},
   "source": [
    "tor.exe -f torrc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1b711d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tor is working.\n"
     ]
    }
   ],
   "source": [
    "%reset -f\n",
    "import requests\n",
    "\n",
    "# Set up Tor proxy\n",
    "tor_proxy = \"socks5://127.0.0.1:9050\"\n",
    "proxies = {\n",
    "    \"http\": tor_proxy,\n",
    "    \"https\": tor_proxy,\n",
    "}\n",
    "\n",
    "# Check if Tor is working by making a request to the official Tor check website\n",
    "try:\n",
    "    response = requests.get(\"https://check.torproject.org\", proxies=proxies)\n",
    "    if \"Congratulations. This browser is configured to use Tor.\" in response.text:\n",
    "        print(\"Tor is working.\")\n",
    "    else:\n",
    "        print(\"Tor is not working.\")\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(\"Tor is not working:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cf671f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install stem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e16094a",
   "metadata": {},
   "source": [
    "Test commente Scrap "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ed22eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to ./scraped_data\\4b506db29e284d3ede8082abc1ba9baa_2025-05-21.txt\n"
     ]
    }
   ],
   "source": [
    "# Ce que fait le code Python (canevas explicatif)\n",
    "\n",
    "# 1. Suppression de toutes les variables en mémoire (utile dans un notebook)\n",
    "%reset -f\n",
    "\n",
    "# 2. Importation des bibliothèques nécessaires\n",
    "from datetime import datetime\n",
    "import os\n",
    "import hashlib\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "# 3. Configuration du proxy Tor\n",
    "# Tor doit être en cours d'exécution sur le port 9050\n",
    "# Le trafic Web passera par le réseau Tor pour garantir l'anonymat\n",
    "\n",
    "# Création d'une URL de proxy SOCKS5 pointant vers le client Tor\n",
    "tor_proxy = \"socks5://127.0.0.1:9050\"\n",
    "\n",
    "# 4. Configuration de Chrome pour utiliser le proxy Tor\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(f\"--proxy-server={tor_proxy}\")\n",
    "\n",
    "# 5. Lancement de Chrome avec les options spécifiées\n",
    "# Utilise WebDriver Manager pour installer automatiquement le bon driver\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "\n",
    "# 6. Navigation vers une publication Facebook précise\n",
    "url_post = \"https://www.facebook.com/Hespress/posts/pfbid021ekyH7772jfUY2QgJm4DUSxjf3EqPrjvHL3851jfASVYuUHRubxmWtustmc8S1Asl\"\n",
    "driver.get(url_post)\n",
    "\n",
    "# 7. Pause manuelle pour permettre à l'utilisateur d'accepter les cookies ou de se connecter si besoin\n",
    "input(\"Press Enter to continue...\")\n",
    "\n",
    "\n",
    "# Click the <div> element with 'Allow all cookies'\n",
    "# allow_cookies_span = driver.find_element(By.XPATH, '//span[contains(text(), \"Allow all cookies\")]')\n",
    "# ActionChains(driver).move_to_element(allow_cookies_span).click().perform()\n",
    "\n",
    "# 8. Cliquer sur tous les boutons \"See more\" pour déployer le texte complet des commentaires\n",
    "see_more_divs = driver.find_elements(By.XPATH, '//div[text()=\"See more\"]')\n",
    "for div in see_more_divs:\n",
    "    div.click()\n",
    "\n",
    "# 9. Attendre 5 secondes pour laisser le temps au DOM de se mettre à jour\n",
    "# (devrait être remplacé par WebDriverWait pour plus de robustesse)\n",
    "time.sleep(5)\n",
    "\n",
    "# 10. Extraction de tous les <div> contenant un style avec \"text-align:\" (correspond aux commentaires)\n",
    "div_elements = driver.find_elements(By.XPATH, '//div[contains(@style, \"text-align:\")]')\n",
    "\n",
    "# 11. Récupération du texte brut de tous ces divs\n",
    "all_text = [div.text for div in div_elements]\n",
    "\n",
    "# 12. Sauvegarde des commentaires extraits dans un fichier texte\n",
    "# Le nom de fichier est basé sur un hash MD5 du contenu + la date\n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "output_dir = \"./scraped_data\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "content_hash = hashlib.md5(\"\\n\".join(all_text).encode()).hexdigest()\n",
    "output_file = os.path.join(output_dir, f\"{content_hash}_{current_date}.txt\")\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(\"\\n\".join(all_text))\n",
    "\n",
    "print(f\"Data saved to {output_file}\")\n",
    "\n",
    "# 13. Fermeture du navigateur\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4217800d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tor version: 0.4.8.16 (git-64ccafd8115ecdec)\n",
      "Tor bytes read: 73165353\n",
      "Tor bytes written: 4618400\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from stem.control import Controller\n",
    "with Controller.from_port(port=9051) as c:\n",
    "    c.authenticate()          # Cookie auth\n",
    "    print(\"Tor version:\", c.get_version())   # devrait afficher la version\n",
    "    print(\"Tor bytes read:\", c.get_info(\"traffic/read\"))\n",
    "    print(\"Tor bytes written:\", c.get_info(\"traffic/written\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50b8c2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install undetected-chromedriver\n",
    "# %pip install facebook-scraper\n",
    "# %pip install requests \n",
    "# %pip install lxml_html_clean\n",
    "# %pip install lxml\n",
    "# %pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b620d759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lxml_html_clean in d:\\pfe\\projet\\fb_scraper\\.fbscrapenv\\lib\\site-packages (0.4.2)\n",
      "Requirement already satisfied: lxml in d:\\pfe\\projet\\fb_scraper\\.fbscrapenv\\lib\\site-packages (from lxml_html_clean) (5.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install lxml_html_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "690c4b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Test IP Tor actuelle…\n",
      "    → IP : 195.47.238.88\n",
      "[+] Collecte des posts de tawjihhoussam via facebook_scraper…\n",
      "✅ 0 URLs enregistrées → scraped_data\\post_links_2025-05-19.csv\n",
      "[+] Fin du script.\n"
     ]
    }
   ],
   "source": [
    "# Scraper URLs Facebook – **facebook‑scraper** + Tor + Cookies  \n",
    "# -------------------------------------------------------------\n",
    "# Objectif : éviter le blocage « You’re Temporarily Blocked » par Facebook.  \n",
    "# Méthode : on abandonne Selenium ; on utilise la librairie `facebook_scraper`,  \n",
    "# qui fait de simples requêtes HTTP (moins détectable).  \n",
    "# On passe toujours par le proxy Tor, ET on fournit ses propres cookies Facebook  \n",
    "# (ainsi le site considère la session comme authentifiée et stable).\n",
    "\n",
    "\"\"\"\n",
    "Étapes préalables :\n",
    "1. Dans Chrome (ou Firefox), connecté à Facebook, exportez les cookies :\n",
    "   – installez l’extension **EditThisCookie** ou **Get cookies.txt**.  \n",
    "   – exportez et enregistrez dans `cookies_fb.txt` (fichier Netscape).  \n",
    "2. Placez ce fichier dans le même répertoire que ce script.\n",
    "3. (Facultatif) Limitez le nombre de pages (`page_limit`) pour rester discret.\n",
    "\"\"\"\n",
    "\n",
    "import csv, datetime, os, time\n",
    "from facebook_scraper import get_posts\n",
    "import requests\n",
    "\n",
    "# ---------- paramètres ----------\n",
    "PAGE_NAME   = \"tawjihhoussam\"        # identifiant de la page\n",
    "PAGE_LIMIT  = 5                       # nombre de pages « scroll » (≈ 50 posts/page)\n",
    "COOKIE_FILE = \"cookies_fb.txt\"       # exporté depuis le navigateur\n",
    "CSV_DIR     = \"scraped_data\"\n",
    "PROXY_TOR   = \"socks5://127.0.0.1:9050\"\n",
    "\n",
    "# ---------- proxies pour facebook_scraper ----------\n",
    "proxies = {\n",
    "    \"http\":  PROXY_TOR,\n",
    "    \"https\": PROXY_TOR,\n",
    "}\n",
    "\n",
    "print(\"[+] Test IP Tor actuelle…\")\n",
    "ip = requests.get(\"https://api.ipify.org\", proxies=proxies, timeout=10).text\n",
    "print(\"    → IP :\", ip)\n",
    "\n",
    "print(f\"[+] Collecte des posts de {PAGE_NAME} via facebook_scraper…\")\n",
    "links = set()\n",
    "for post in get_posts(PAGE_NAME, cookies=COOKIE_FILE, extra_info=False,\n",
    "                      page_limit=PAGE_LIMIT, proxies=proxies):\n",
    "    links.add(post[\"post_url\"].split(\"?\", 1)[0])\n",
    "    time.sleep(2)   # 1 requête toutes les 2 s ≈ 30/h (reste sous le rate‑limit)\n",
    "\n",
    "os.makedirs(CSV_DIR, exist_ok=True)\n",
    "csv_path = os.path.join(CSV_DIR, f\"post_links_{datetime.date.today()}.csv\")\n",
    "with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    csv.writer(f).writerows([[\"url\"]] + [[u] for u in sorted(links)])\n",
    "\n",
    "print(f\"✅ {len(links)} URLs enregistrées → {csv_path}\")\n",
    "print(\"[+] Fin du script.\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17c76eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Test IP Tor actuelle…\n",
      "    → IP : 195.47.238.88\n",
      "[+] Collecte des posts de tawjihhoussam via facebook_scraper…\n",
      "✅ 0 URLs enregistrées → scraped_data\\post_links_2025-05-19.csv\n"
     ]
    }
   ],
   "source": [
    "# Scraper URLs Facebook – **facebook‑scraper** + Tor + Cookies  \n",
    "# -------------------------------------------------------------\n",
    "# Objectif : éviter le blocage « You’re Temporarily Blocked » par Facebook.  \n",
    "# Méthode : on abandonne Selenium ; on utilise la librairie `facebook_scraper`,  \n",
    "# qui fait de simples requêtes HTTP (moins détectable).  \n",
    "# On passe toujours par le proxy Tor, ET on fournit ses propres cookies Facebook  \n",
    "# (ainsi le site considère la session comme authentifiée et stable).\n",
    "#\n",
    "# ⚠️ Dépendances minimales :\n",
    "#   pip install \"facebook_scraper[html]\"    # installe aussi lxml_html_clean\n",
    "#   OU bien :  pip install facebook_scraper lxml_html_clean requests\n",
    "#\n",
    "# ────────────────────────────────────────────────────────────────────────\n",
    "\"\"\"\n",
    "Étapes préalables :\n",
    "1. Dans Chrome (ou Firefox), connecté à Facebook, exportez les cookies :\n",
    "   – installez l’extension **EditThisCookie** ou **Get cookies.txt**.  \n",
    "   – exportez et enregistrez dans `cookies_fb.txt` (fichier Netscape).  \n",
    "2. Placez ce fichier dans le même répertoire que ce script.\n",
    "3. (Facultatif) Limitez le nombre de pages (`PAGE_LIMIT`) pour rester discret.\n",
    "\"\"\"\n",
    "\n",
    "import csv, datetime, os, time, subprocess, sys\n",
    "\n",
    "# ----- installer lxml_html_clean à la volée si absent -----\n",
    "try:\n",
    "    from lxml.html.clean import Cleaner  # noqa: F401 (test import)\n",
    "except ImportError:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"lxml_html_clean\"])\n",
    "\n",
    "from facebook_scraper import get_posts\n",
    "import requests\n",
    "\n",
    "# ---------- paramètres ----------\n",
    "PAGE_NAME   = \"tawjihhoussam\"        # identifiant de la page\n",
    "PAGE_LIMIT  = 5                       # nombre de pages « scroll » (≈ 50 posts/page)\n",
    "COOKIE_FILE = \"cookies_fb.txt\"       # exporté depuis le navigateur\n",
    "CSV_DIR     = \"scraped_data\"\n",
    "PROXY_TOR   = \"socks5://127.0.0.1:9050\"\n",
    "\n",
    "proxies = {\n",
    "    \"http\":  PROXY_TOR,\n",
    "    \"https\": PROXY_TOR,\n",
    "}\n",
    "\n",
    "print(\"[+] Test IP Tor actuelle…\")\n",
    "try:\n",
    "    ip = requests.get(\"https://api.ipify.org\", proxies=proxies, timeout=10).text\n",
    "    print(\"    → IP :\", ip)\n",
    "except Exception as e:\n",
    "    print(\"    Impossible de récupérer l'IP via Tor :\", e)\n",
    "\n",
    "print(f\"[+] Collecte des posts de {PAGE_NAME} via facebook_scraper…\")\n",
    "links = set()\n",
    "for post in get_posts(PAGE_NAME, cookies=COOKIE_FILE, extra_info=False,\n",
    "                      page_limit=PAGE_LIMIT, proxies=proxies):\n",
    "    links.add(post[\"post_url\"].split(\"?\", 1)[0])\n",
    "    time.sleep(2)   # ≈30 requêtes/h, on reste loin du blocage\n",
    "\n",
    "os.makedirs(CSV_DIR, exist_ok=True)\n",
    "csv_path = os.path.join(CSV_DIR, f\"post_links_{datetime.date.today()}.csv\")\n",
    "with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    csv.writer(f).writerows([[\"url\"]] + [[u] for u in sorted(links)])\n",
    "\n",
    "print(f\"✅ {len(links)} URLs enregistrées → {csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aece0fa6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".fbscrapenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
